%https://www.amazon.de/dp/069107951X/ref=sr_1_1?dchild=1&keywords=9780691079516&qid=1584700659&sr=8-1
%  author={Bellman, R. and Rand Corporation and Karreman Mathematics Research Collection},
\textcite{bellman1957dynamic}[20-22] first introduces the \textit{curse of dimensionality}. The curse effects a mathematical model, when there are a large number of variables. The real world is complicated and by trying to incorporate as many real world features into a mathematical model, it becomes complicated. A too simple model however will not be suitable for prediction.

% Many fields share the same problem, that the expansion of exceedingly difficult problems have gone beyond the confines of conventional mathematical theory
% number of variables is large
% portent: a sign or warning that a momentous or calamitous event is likely to happen.

%https://books.google.at/books?hl=en&lr=&id=iwbWCgAAQBAJ&oi=fnd&pg=PR9&dq=Adaptive+Control+Processes&ots=bDJ9XsIa7c&sig=Rc5D_JAZc7a4FigIbt3DmiNYhJI#v=snippet&q=curse%20of%20dimensionality&f=false
\textcite{bellman2015adaptive}[94] further effects the results of \textit{the curse of dimensionality}. Functions with one variable can be visualised as curve in a 2D space and a function with two variables in a 3D space. Depicting functions with more variables is however more problematic (both for visualisation and tabulation). According to \textcite{DataMiningAndPredictiveAnalytics}[92, 93 ???TODO CHECK WHICH PAGE], high quality visualisation methods usually cannot depict more than five dimensions.
%TODO: not sure if i've understood this next bit right:
\textcite{bellman2015adaptive}[94, 198] gives as an example, imagine, for example, if the variables of a function take on the values between 1 and 100. While a function with one variable would need to tabulate 100 values, a function with 2 variables would need to tabulate 100 x 100 = 10\textsuperscript{4} values and a function with 3 variables 10\textsuperscript{6}. Each additional variable adds more complexity to the 

% Multidimensional variational problems require more complex methods to solve them. 

% Imagine, for example, the variables are defined between 0 and 1 (e.g. u(t), where 0 $\leq$ t $\leq$ 1), at the points k(0.01). A function with one variable would require 100 values (the variable would take the numbers 0 to 100) to be tabulated. 






%TODO: FIND OUT WHO FIRST SUGGESTED IT FOR DATA MINING? ALTHOUGH DON'T THINK I'LL FIND MUCH
According to \textcite{DataMiningAndPredictiveAnalytics}[92, 93], a high amount of predictor variables in data mining can lead to overfitting and overlooking crucial relationships between predictors. Dimensionality reduction techniques have the ability to reduce the number of predictor items, aid in ensuring that these predictor items are independent, and present a framework for interpretability of the results.
As stated by \textcite{han2011data}[93], dimensionality reduction is a data reduction method. Data reduction is utilised to attain a smaller, more concentrated data set, whilst mostly keeping the integrity of the initial data set. 

%Analysis of a complex of statistical variables by harold hotelling
%https://babel.hathitrust.org/cgi/pt?id=wu.89097139406&view=1up&seq=20
%On lines and planes of closest fit to systems of points in space
%-pdf

Principal Component Analysis was first proposed by \textcite{OnLinesAndPlanes1901} and \textcite{hotelling1933analysis}.
Pearson's approach is to identify a line or plane that best fits the collected variables plotted to a plane. In order to determine the best fitting line or plane, means, standard-deviations, and correlations are used \autocite{OnLinesAndPlanes1901}[559-560].
\textcite{hotelling1933analysis}[4, 5, 10, 15, 18, 24-25] introduces his method as \textit{the method of principal components}. 
%TODO: NOT SURE WHAT n IS - PAGE 1 SAYS NUMBER OF VARIABLES, BUT NOT SURE IF THAT REALLY MAKES SENCE HERE - pretty sure its dimensions, mentioned in next paragraph:
When choosing the calculated components, they are chosen with the decreasing amount of variance. Therefore, the one with the highest variance (\textit{$\gamma$\textsubscript{1}}) is chosen first. The next highest (\textit{$\gamma$\textsubscript{2}}) is chosen orthogonal to \textit{$\gamma$\textsubscript{1}} and so on, until the number \textit{n} dimensions are reached (\textit{$\gamma$\textsubscript{n}}). The components left with small variance are disregarded, since they are trivial. The method results in selecting a new set of coordinate axes which correspond to the principal axes of ellipsoids. 
%NOT SURE IF UNDERSTOOD THIS RIGHT, END PAGE 24&25:
In an example, Helling selects the two of four calculated principal components with the highest variance, therefore being able to display the data on a two dimensional scatter diagram. (?)
%10
Geoemtrically, the new coordinate axes are rotated to lie along the principal axes of the ellipsoids.
%There must be a distance metric (e.g. Euclidean) must be determined in the \textit{n}-dimensional space. - something about unit, maybe unit vector but not sure

%statistical variables x, x's are correlated, might be some y'S (independent variables) that determine the values x will take


%DELELTE Pearson (1901), on the other hand, was concerned with finding lines and planes that best fit a set of points in p-dimensional space, and the geometric optimization problems he considered also lead to PCs, as will be explained in Section 3.2



\textcite{han2011data}[93, 95-96] stated, in data mining the vectors with the lowest variance that are removed, reduce the amount of data and number of dimensions. Despite the loss of data, the components with higher variance can approximate the original data. The authors suggested wavelet transforms (e.g. discrete wavelet transform (DWT)) as another method of dimensionality reduction.


% \textcite{han2011data}[95-96]
% \begin{enumerate}
%   \item The first step is to standardise the input data, therefore making the data-range identical. \textcite{DataMiningAndPredictiveAnalytics}[94] declares, that after standardising the data, the mean is zero and the standard deviation is one.
%   \item Next, k orthonormal vectors are calculated, the so called \textit{principal components}. These unit vectors present a basis for the input data, which are a linear combination of the principal components. \textcite{DataMiningAndPredictiveAnalytics}[94] explain, that the principal components can be discovered, by rotating the initial coordinate system to the direction of maximum variability. These then create a new coordinate system. 
%   \item In the following step, as stated by \textcite{han2011data}[95-96], the principal components are put into order by their decreasing significance/strength, thus presenting their variance. These vectors are used as new axes for the data, the first axis exhibits the highest variance.
%   \item Due to the decreasing order  of variance, the vectors with the lowest variance can be removed, therefore reducing the amount of data and number of dimensions. Despite the loss of data, the components with higher variance can approximate the original data.
% \end{enumerate}

%TODO: EXPLAIN MORE IN DETAIL!! ALTHOUGH - MIGHT BE DONE IN PRACTICAL PART - ASK

%TODO: explain what variance is, maybe also skewed data
%variance = range, want data as spread out as we can find it
%either, find highest variance (where points are spread out the most), or minimum error(distance to axis)
%retain most of the information
%since eigenvalues - each principal component ends up orthogonal
%youtuber video: which direction will you pick, that maximizes the variance
%page 93
