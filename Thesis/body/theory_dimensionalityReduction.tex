%https://www.amazon.de/dp/069107951X/ref=sr_1_1?dchild=1&keywords=9780691079516&qid=1584700659&sr=8-1
%  author={Bellman, R. and Rand Corporation and Karreman Mathematics Research Collection},
\textcite{bellman1957dynamic}[20-22] first introduces the \textit{curse of dimensionality}. The curse effects a mathematical model, when there are a large number of variables. The real world is complicated and by trying to incorporate as many real world features into a mathematical model, it becomes complicated. A too simple model however will not be suitable for prediction.

% Many fields share the same problem, that the expansion of exceedingly difficult problems have gone beyond the confines of conventional mathematical theory
% number of variables is large
% portent: a sign or warning that a momentous or calamitous event is likely to happen.

%https://books.google.at/books?hl=en&lr=&id=iwbWCgAAQBAJ&oi=fnd&pg=PR9&dq=Adaptive+Control+Processes&ots=bDJ9XsIa7c&sig=Rc5D_JAZc7a4FigIbt3DmiNYhJI#v=snippet&q=curse%20of%20dimensionality&f=false
\textcite{bellman2015adaptive}[94] further effects the results of \textit{the curse of dimensionality}. Functions with one variable can be visualised as curve in a 2D space and a function with two variables in a 3D space. Depicting functions with more variables is however more problematic (both for visualisation and tabulation). According to \textcite{DataMiningAndPredictiveAnalytics}[92, 93 ???TODO CHECK WHICH PAGE], high quality visualisation methods usually cannot depict more than five dimensions.
%TODO: not sure if i've understood this next bit right:
\textcite{bellman2015adaptive}[94, 198] gives as an example, imagine, for example, if the variables of a function take on the values between 1 and 100. While a function with one variable would need to tabulate 100 values, a function with 2 variables would need to tabulate 100 x 100 = 10\textsuperscript{4} values and a function with 3 variables 10\textsuperscript{6}. Each additional variable adds more complexity to the 

% Multidimensional variational problems require more complex methods to solve them. 

% Imagine, for example, the variables are defined between 0 and 1 (e.g. u(t), where 0 $\leq$ t $\leq$ 1), at the points k(0.01). A function with one variable would require 100 values (the variable would take the numbers 0 to 100) to be tabulated. 






%TODO - Data Preparation for Data Mining PAGE 329 (but actually page 355)

%book data mining and predictive analysis
%TODO: CHECK IF OK TO DO THIS WITH BELLMAN: CAUSE DON'T HAVE THE PAPER - maybe better with footnote
%\textcite{DataMiningAndPredictiveAnalytics}[92, 93]: Furthermore, visualising higher dimensions becomes more challenging.  Humans use these visualisations for visual pattern recognition. 
According to \textcite{DataMiningAndPredictiveAnalytics}[92, 93], a high amount of predictor variables in data mining can lead to overfitting and overlooking crucial relationships between predictors. Dimensionality reduction techniques have the ability to reduce the number of predictor items, aid in ensuring that these predictor items are independent, and present a framework for interpretability of the results.
As stated by \textcite{han2011data}[93], dimensionality reduction is a data reduction method. Data reduction is utilised to attain a smaller, more concentrated data set, whilst mostly keeping the integrity of the initial data set. 

%Analysis of a complex of statistical variables by harold hotelling
%https://babel.hathitrust.org/cgi/pt?id=wu.89097139406&view=1up&seq=20

%Data mining concepts and techniques
%also page 93
% As stated by \textcite{han2011data}[93, 95-96], dimensionality reduction is a data reduction method. Data reduction is utilised to attain a smaller, more concentrated data set, whilst mostly keeping the integrity of the initial data set. 
% \textcite{han2011data}[93, 95-96]
% PCA projects the initial data onto a smaller space, thus removing random variables. The data it is applied to can be ordered or unordered, sparse and skewed. PCA is conducted in the following steps:

Principal Component Analysis was first proposed by \textcite{OnLinesAndPlanes1901} and \textcite{hotelling1933analysis}.

\textcite{han2011data}[93, 95-96]
\begin{enumerate}
  \item The first step is to standardise the input data, therefore making the data-range identical. \textcite{DataMiningAndPredictiveAnalytics}[94] declares, that after standardising the data, the mean is zero and the standard deviation is one.
  \item Next, k orthonormal vectors are calculated, the so called \textit{principal components}. These unit vectors present a basis for the input data, which are a linear combination of the principal components. \textcite{DataMiningAndPredictiveAnalytics}[94] explain, that the principal components can be discovered, by rotating the initial coordinate system to the direction of maximum variability. These then create a new coordinate system. 
  \item In the following step, as stated by \textcite{han2011data}[95-96], the principal components are put into order by their decreasing significance/strength, thus presenting their variance. These vectors are used as new axes for the data, the first axis exhibits the highest variance.
  \item Due to the decreasing order  of variance, the vectors with the lowest variance can be removed, therefore reducing the amount of data and number of dimensions. Despite the loss of data, the components with higher variance can approximate the original data.
\end{enumerate}

%TODO: EXPLAIN MORE IN DETAIL!! ALTHOUGH - MIGHT BE DONE IN PRACTICAL PART - ASK

%TODO: explain what variance is, maybe also skewed data
%variance = range, want data as spread out as we can find it
%either, find highest variance (where points are spread out the most), or minimum error(distance to axis)
%retain most of the information
%since eigenvalues - each principal component ends up orthogonal
%youtuber video: which direction will you pick, that maximizes the variance
%page 93
Other methods of dimensionality reduction include wavelet transforms (e.g. discrete wavelet transform (DWT)).