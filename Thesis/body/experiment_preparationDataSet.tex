Initially, the raw data in the .csv files was distributed in multiple files (one file per user, per time interval). The files were read in and processed by the Python Pandas\footnote{\url{https://pandas.pydata.org}} tool. The library offers fast and flexible functionalities for data analysis. Using the Pandas read\_csv and concat methods, the files with identical time periods were transformed and concatenated to one collective DataFrame. According to Pandas' documentation, DataFrames are fast and efficient 2D data structures, used to store tabular data. 


\subsubsection{Missing Values}
The raw data contained several rows with empty cells. Section \ref{section:MissingValues} lists approaches on how to substitute missing data, thus preserving the row, which could otherwise contain important information. In doing so however, the existing patterns could be disrupted. In order maintain the data's patterns, rows with missing values were removed. Using the Pandas' dropna (deletes rows with missing values), all rows containing empty cells were dropped. Of the originally 8283 rows from the 1 hour time period files, only 3279 (39.59\%) rows were complete and remained. In the 3 hour files, only 6218 from 14091 (44.13\%) persisted.

\subsubsection{Normalisation}
The data recorded from the different smartphone sensors returned values with different ranges. For example, whilst the values that describe the screen on time range between -20 and 2.0, the light sensor values can range from 0 to over 6000. As explained in \ref{section:Normalisation}, values with higher ranges can inadvertently outweigh smaller values. To be sure that the values are weighted the same, \textit{Min-Max Normalization} was used to map all the values into the common range [0,1]. The sklearn MinMaxScaler was used to calculate the new, normalised values in the data set.

\subsubsection{Removal of unnecessary columns}
TODO:
Also - compressing 1-N columns
The deletion of rows with missing values led to the loss of over 50\% of the rows...


In order to check the functionality of the data preprocessing functions, python tests were written using random mock data.