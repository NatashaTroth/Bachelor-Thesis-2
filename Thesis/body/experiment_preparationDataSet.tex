Initially, the raw data in the .csv files was distributed in multiple files (one file per user, per time interval). The files were read in and processed by the Python Pandas\footnote{\url{https://pandas.pydata.org}} tool. The library offers fast and flexible functionalities for data analysis. Using the Pandas read\_csv and concat methods, the files with identical time periods were transformed and concatenated to one collective DataFrame. According to Pandas' documentation, DataFrames are fast and efficient 2D data structures, used to store tabular data. 


\subsubsection{Missing Values}
The raw data contained several rows with empty cells. Section \ref{section:MissingValues} lists approaches on how to substitute missing data, thus preserving the row, which could otherwise contain important information. In doing so however, the existing patterns could be disrupted. In order maintain the data's patterns, rows with missing values were removed. Using the Pandas' dropna (deletes rows with missing values), all rows containing empty cells were dropped. Of the originally 8283 rows from the 1 hour time period files, only 3279 (39.59\%) rows were complete and remained. In the 3 hour files, only 6218 from 14091 (44.13\%) persisted.

\subsubsection{Normalisation}
The data recorded from the different smartphone sensors returned values with different ranges. For example, whilst the values that describe the screen on time range between -20 and 2.0, the light sensor values can range from 0 to over 6000. As explained in \ref{section:Normalisation}, values with higher ranges can inadvertently outweigh smaller values. To be sure that the values are weighted the same, \textit{Min-Max Normalization} was used to map all the values into the common range, e.g. [0,1]. The sklearn MinMaxScaler was initially used to calculate the new, normalised values in the data set. Most of the values in the data set ranged between 0 and just over 100. The light sensor values were the exception, with its values reaching up above 50,000. As mentioned in section \ref{section:Normalisation}, Z-score normalization is more robust to outliers, that could otherwise bias Min-Max Normalization. Therefore, the sklearn StandardScaler was used instead of the MinMaxScaler, which implements the Z-score normalization.

\subsubsection{Selection of columns (attributes)}
TODO:
Also - compressing 1-N columns



%tod0: say with min max - high value, rest squished together
\subsubsection{Feature reduction}
\textbf{todo - finish or remove}
The deletion of rows with missing values led to the loss of over 50\% of the rows in all time periods. With the intention to preserve as many of these rows as possible, columns with equal to or more than 30\% of rows with missing values were removed. This resulted in only 4 to 5 features of the original 8 being sustained. The results appeared to be unchanged and so, this feature reduction was eventually removed.


% In order to check the functionality of the data preprocessing functions, python tests were written using random mock data.
