% in clustering after dim red

Dimensionality reduction was implemented to reduce the number of dimensions (number of attributes, so in this case number of columns). Principal Components Analysis (PCA) and t-SNE, as described in section \ref{section:DimensionalityReduction}, were used to reduce the dimensionality of the data set. PCA was the initial approach used in the experiment. The sklearn PCA\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html}} function was used to reduce the number of dimensions to 2, which simplified visualisation in 2D scatterplots. The PCA allowed 65\%-95\% (depending on data preparation type) of the data's important structures to be accounted for in only the first two or three principle components. As can be seen in figure \textbf{TODO: figures}, the resulting data from these components, didn't show any significant clusters, in comparison to the t-SNE results.
%&todo - this is because not linear data and pca is linear.
%todo - mention 3D or just stick to 2D

The t-SNE approach proved to be more significant.

Since the results 


\subsubsection{Find t-SNE parameters}
% \textcite{tsneAutomaticPerplexity}[1-2] propose a method to automatically select the t-SNE parameter perplexity.\textbf{WHAT IS THE PERPLEXITY} This parameter influences the results received from t-SNE and is difficult to optimise for inexperienced users. The authors explain, that the Kullback-Leibler (KL) divergences received from different complexities cannot solely be compared to find the most suitable perplexity. As the perplexity rises, the KL falls, selecting the smallest KL would therefore result in a very high perplexity (e.g. 400), whereas \textcite{maaten2008visualizing}[2582] recommend setting the perplexity to between 5 and 50. \textcite{tsneAutomaticPerplexity}[2-] further explains, that when the perplexity set to the number of data points, no interesting structure is found, only a Gaussian or uniform like blob. Accordingly, it could be perceived, that a trade off between the final Kullback-Leibner divergence and the Perplexity (\textit{Perp}) might result in good t-SNE results. The authors propose the following formula:

% \[
%   S(Perp) = KL(P||Q) + log(n)\frac{Perp}{n}
% \]

% Its results - human experts agree
