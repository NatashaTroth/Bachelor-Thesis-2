In his book, \textcite{mccue2014data}[53] mentions, that generally in data mining, the analysis itself requires 20\% of the time. The other 80\% is the preparation of the data.

To make data useful in data mining, \textcite{DataMiningAndPredictiveAnalytics}[20] point out, that datasets first need to undergo a data preprocessing step. Raw data extracted directly from databases can be incomplete (values are missing), noisy (contains outliers), or may contain out-dated or redundant data. \textcite{dataPreprocessingInDataMining}[45] state, that dirty data can impact the produced model, making it less reliable. The significance of its effect depends on the implemented data mining method. \textcite{DataMiningAndPredictiveAnalytics}[20, 45] define, the goal is to decrease the irrelevant data that is fed into the model, thus reducing the amount of irrelevant data received out of the model. The authors also report, that ID fields should be removed from the dataset used in data mining algorithms, since the value is different for each record and not helpful. It could however prove harmful, since a relationship might be presumed that isn't there. Further information preparation steps are outlined in the next sections below.



\subsubsection{Missing values}
\label{section:MissingValues}

According to \textcite{dataPreparationForDataMining}[83], it is good practice to differentiate "empty" values from "missing" ones. Empty values do not have a comparable real-world value. Missing values, however, do have underlying values, they simply weren't recorded. Removing the record with the missing value would mean wasting the data stored in the other fields of that record, that might contain relevant information. Substituting the value means, that the record can be used. A consideration is how to substitute the missing value, without adding bias to the dataset. An inadequately chosen replacement value could distort the dataset, by adding data which doesn't exist in the real world. \textcite{DataMiningAndPredictiveAnalytics}[23, 25] give an example, of how replacing missing values can lead to invalid results. The authors experimented with a database of cars. Substituting a missing brand with a random value (here "Japan") led to a car, that didn't even exist. Data imputation takes into account the other attributes stored in the record and from these, calculates what the missing value would most likely be. Larose and Larose suggest, that the value can be replaced, either with a constant determined by the data analyst, with a field mean (for numerical values) or mode (for categorical values), with a random value, or with imputed values based on the different features of the record.  
  



\subsubsection{Normalisation}
\label{section:Normalisation}

\textcite{han2011data}[105] describe normalisation as giving the attributes of a dataset equal weight. For example, it can transform the data to fall in a smaller, common range (e.g. [-1, 1]). It therefore hinders variables with large ranges from outweighing ones with smaller ranges. People's income would, for instance, have a larger range than binary attributes. 

\textcite{dataPreprocessingInDataMining}[46-48] review the following normalisation methods: \textit{min-max normalization}, \textit{z-score normalization}, and \textit{decimal scaling normalization}. For the following examples, \textit{A} is a numerical attribute from a dataset, a single value of this attribute is represented with \textit{v}:
\begin{itemize}
  \item Min-max normalization scales the original numerical values to a newly defined range, with a new minimum (\textit{newMin\textsubscript{A}}) and maximum (\textit{newMax\textsubscript{A}}) (e.g. 0.0 and 1.0). The original minimum and maximum values found in \textit{A} are presented as \textit{min\textsubscript{A}} and \textit{max\textsubscript{A}} respectively:
  \[
    v' = \frac{v - min_A}{max_A - min_A}(newMax_A - newMin_A) + newMin_A
  \]
  The intervals [0, 1] and [-1, 1] are common intervals for normalisation. 

  \item Z-score (or zero-mean) normalization normalises the values using the mean (\textit{\={A}}) and standard deviation \textit{$\sigma$\textsubscript{A}} of the values \textit{A}.
  \[
    v' = \frac{v - \overline{A}}{\sigma_A}
  \]
  After this transformation, the mean equals zero and the standard deviation is one. The advantages of this normalisation method take effect, when the min and max values of \textit{A} are not known, or when there are outliers that could bias the min-max method.

  \item Decimal scaling is another normalisation method.
 
\end{itemize}
