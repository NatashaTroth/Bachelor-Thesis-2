

To make data useful in data mining, \textcite{DataMiningAndPredictiveAnalytics}[20] point out, that data sets first need to undergo a data preprocessing step, including data cleaning and data transformation. Raw data extracted directly from databases can be incomplete (values are missing), noisy (contains outliers), or may contain out-dated or redundant data. \textcite{dataPreprocessingInDataMining}[45] list the following sources of dirty data: data entry errors, data update errors, data transmission errors and bugs. Dirty data can impact the produced model, making it less reliable. The significance of its effect depends on the implemented data mining method. \textcite{DataMiningAndPredictiveAnalytics}[20] define the goal as to decrease garbage in, garbage out (GIGO). To clarify, decreasing \textit{garbage in} means to reduce the irrelevant data that is fed into the model, thus reducing the amount of irrelevant data received out of the model (\textit{garbage out}). Further information to different types of dirty data and their solutions are outlined in the next sections below.

%datamining and pred. analy. p20: incomplete (values are missing) or be noisy (contains outliers), or may contain out-dated or redundant data.  This unpreprocessed data also may not be in a correct form for data mining models.

%TODO: maybe min max normalisation if can't find any other source


\subsubsection{Noisy Data}
\label{section:NoisyData}
\textcite{dataPreparationForDataMining}[71-72] interprets outliers as objects that have low recurrence and that are separated from the main collection of values. These values are often mistakes and can lead to distortion of the data set. Insurance companies provide a good example of outliers. The majority of insurance claims are only for a small sum, however every so often a customer may be in need of a large claim (outlier). \textcite{han2011data}[28-29] mention, that in some cases, these uncommon events are of more interest. One of these instances is detecting unusually large payments compared to the card holders normal payments, to uncover fraudulent usage of credit cards. As stated by \textcite{DataMiningAndPredictiveAnalytics}[26-27], there are some data mining algorithms that have trouble functioning correctly when fed outliers. Moreover, outliers may be data errors. Graphical methods used to identify outliers include, histograms or two-dimensional scatter plots. In order to smooth the data, \textcite{han2011data}[84] use binning, regression and outlier analysis (e.g. clustering).
%TODO: EXPLAIN???
% \begin{itemize}
%   \item Binning: The values are sorted into bins, according to their neighbours. In smoothing by means, each value in the bin is replaced by the bin's mean (or median) value. In smoothing by boundaries, each value is substituted with either the minimum value in the bin, or the maximum, whichever value is closer.
%   \item Regression: 
% \end{itemize}

% \textcite{DataMiningAndPredictiveAnalytics}[38] 
% The Z-score method can be used to numerically calculate outliers. Outliers should not automatically be removed from the data set.



% ------------- MISSING VALUES ---------------
\subsubsection{Missing Values}
\label{section:MissingValues}

%81 - PROVIDE THIS PDF - CAUSE CAN'T FIND ACTUAL COPY SO MIGHT BE WRONG PAGE NUMBERs - MAYBE MENTION THE CHAPTERS?
According to \textcite{dataPreparationForDataMining}[81-82, 260, 264, 267], it is good practice to differentiate "empty" values from "missing" ones. Empty values do not have a comparable real-world value. Missing values, however, do have underlying values, they simply weren't recorded. The author does not recommend ignoring the record with the missing value, since it would mean wasting the data stored in the other fields of that record. These fields may contain relevant information. Substituting the value, means that the record can be used. One of the problems with not having these values, is that this missing information content (e.g. predictive or inferential) could be carried by the pattern. Another consideration is how to substitute the missing value, without adding bias to the data set. An inadequately chosen replacement value could distort the data set, by adding data which doesn't exist in the real world. A crucial focus is reserving the relationship between variables.  Substitution of values, if not suitable, may disrupt the between-variable variability, thus hiding or distorting patterns in the data. 

\textcite{DataMiningAndPredictiveAnalytics}[23, 25] give an example, of how replacing missing values can lead to invalid results. The authors experimented with a database of cars. Substituting a missing brand with a random value (here "Japan") led to a car, that didn't even exist. Data imputation takes into account the other attributes stored in the record and from these, calculates what the missing value would most likely be. Larose and Larose suggest, that the value can be replaced, either with a constant determined by the data analyst, with a field mean (for numerical values) or mode (for categorical values), with a random value, or with imputed values based on the different features of the record.  
\textcite{dataPreparationForDataMining}[260, 267-269] points out, that regression can be used to find supplement values. When using regression (e.g. linear regression), one can calculate a value with the help of another given value. There are several different methods to replace missing values. Some of which promise to generate more information. Such methods however are computationally complex.  


%%for more on data imputation - see chapter 27 

% \textcite{DataMiningAndPredictiveAnalytics}[25] 
% Another step in data preprocessing is identifying missclassifications. An example given by the authors, is classifying a record as USA instead of US, or France instead of Europe. These classes only contained one record in comparison to the other more frequently used classes.

%page 27 - chapter measures of center and spread - not sure if need - mean, median and mode

\subsubsection{Normalisation}
\label{section:Normalisation}

%TODO: ADD GRAPHS, CAN ALSO BE FROM A WEBSITE, E.G. https://www.codecademy.com/articles/normalization

% \textcite{DataMiningAndPredictiveAnalytics}[130-31] 
% In some data mining algorithms, variables with higher ranges can unjustly influence the results, having more influence than smaller ones. Therefore, the authors recommend to normalise numerical data. 

%THIS IS ALSO EXPLAINED BY DATA PROCESSING IN DATA MINING ON PAGE 47
\textcite{han2011data}[105-106] describes normalisation as giving the attributes of a data set equal weight. For example, it can transform the data to fall in a smaller, common range (e.g. [-1, 1]). It therefore hinders variables with large ranges from outweighing ones with smaller ranges. Income would, for instance, have a larger range than binary attributes. 

% %data mining concepts and techniques
\textcite{dataPreprocessingInDataMining}[46-48] explain, that raw data is often transformed to produce new attributes with more applicable properties in the process of normalisation. These new attributes are then known as \textit{modeling variables} or \textit{analytic variables}. 
\textit{Min-Max Normalization}, \textit{Z-score Normalization}, and \textit{Decimal Scaling Normalization} are methods that convert the distribution of the existing attributes. For the following examples, \textit{A} is a numerical attribute from a data set, a single value of this attribute is represented with \textit{v}:
\begin{itemize}
  \item Min-max normalization scales the original numerical values to a newly defined range, with a new minimum (\textit{newMin\textsubscript{A}}) and maximum (\textit{newMax\textsubscript{A}}) (e.g. 0.0 and 1.0). The original minimum and maximum values found in \textit{A} are presented as \textit{min\textsubscript{A}} and \textit{max\textsubscript{A}} respectively:
  \[
    v' = \frac{v - min_A}{max_A - min_A}(newMax_A - newMin_A) + newMin_A
  \]
  The intervals [0, 1] and [-1, 1] are common intervals for normalisation. 
  % If new data is added, that isn't within the min and max of \textit{A} range, an "out-of-bounds" error will occur.

  \item Z-score (or zero-mean) normalization normalises the values using the mean (\textit{\={A}}) and standard deviation \textit{$\sigma$\textsubscript{A}} of the values \textit{A}.
  \[
    v' = \frac{v - \overline{A}}{\sigma_A}
  \]
  After this transformation, the mean equals zero and the standard deviation is one. The advantages of this normalisation method take effect, when the min and max values of A are not known, or when there are outliers that could bias the min-max method.

  \item The decimal scaling method moves the decimal point enough spaces, so that the maximum absolute attribute value of \textit{A} is below one. The smallest required number of digits to move the decimal point, so that the largest absolute number in \textit{A} is below zero, is represented by \textit{j}:
  \[
    v' = \frac{v}{10^j}
  \]
\end{itemize}



\subsubsection{Data Transformation}
\label{section:Transformation}

\textbf{Todo: complete (if needed)}
% \textbf{NOT REALLY SURE WHAT TO DO HERE - LEAVE OPEN, WAIT AND SEE WHAT HAVE TO DO IN THE EXPERIMENT}
%not sure if should explain these more, page 32 has Decimal Scaling.

%should state the difference between categorical and numerical data - maybe page 333
%page 39-41
As reported by \textcite{DataMiningAndPredictiveAnalytics}[39-41, 45], flag variables can be used to transform categorical variables into numerical. A flag variable can take on one of two values: 0 and 1 (e.g. female = 0, male = 1). When k $\geq$ 3 (k being the amount of categorical predictors), the variables can be transformed into k-1 flag variables. Assigning categorical variables numerical values is not advised, since this orders the categorical variables. For example, if North = 1, East = 2, South = 3 and West = 4, West would be closer to South than to North, etc.


%page 45
ID fields should be removed from the dataset, since the value is different for each record and not helpful.

%OTHER TOPICS - NOT SURE IF NEED:
%page 38- z-score for detecting outliers, like in boxplot
%page 39 - Flag Variables
%page 40 - Transforming categorical variables into numerical variables
%page 41 - Binning numerical variables
%page 56 - exploring categorical variables
%page 64 - exploring numerical variables


% \subsubsection{Something, not sure what yet}

