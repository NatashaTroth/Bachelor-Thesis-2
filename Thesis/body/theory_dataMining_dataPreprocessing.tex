In his book, \textcite{mccue2014data}[53] mentions, that generally in data mining, the analysis itself on requires 20\% of the time. The other 80\% is the preparation of the data.
%https://books.google.at/books?hl=en&lr=&id=re1MBAAAQBAJ&oi=fnd&pg=PP1&dq=Data+Mining+and+Predictive+Analysis:+Intelli-+gence+Gathering+and+Crime+Analysis&ots=loCA-gPHIn&sig=UNrfEqV-WEn-mQc7DPoeW_h8F24&redir_esc=y#v=onepage&q=80&f=false

To make data useful in data mining, \textcite{DataMiningAndPredictiveAnalytics}[20] point out, that datasets first need to undergo a data preprocessing step. Raw data extracted directly from databases can be incomplete (values are missing), noisy (contains outliers), or may contain out-dated or redundant data. \textcite{dataPreprocessingInDataMining}[45] states, that dirty data can impact the produced model, making it less reliable. The significance of its effect depends on the implemented data mining method. \textcite{DataMiningAndPredictiveAnalytics}[20, 45] define the goal as to decrease garbage in, garbage out (GIGO). To clarify, decreasing \textit{garbage in} means to reduce the irrelevant data that is fed into the model, thus reducing the amount of irrelevant data received out of the model (\textit{garbage out}). The authors also report, that ID fields should be removed from the dataset used in data mining algorithms, since the value is different for each record and not helpful. It could however prove harmful, since a relationship might be presumed that isn't there. Further information preparation steps are outlined in the next sections below.




% ------------- MISSING VALUES ---------------
\subsubsection{Missing Values}
\label{section:MissingValues}

%81 - PROVIDE THIS PDF - CAUSE CAN'T FIND ACTUAL COPY SO MIGHT BE WRONG PAGE NUMBERs - MAYBE MENTION THE CHAPTERS?
According to \textcite{dataPreparationForDataMining}[81-82, 260, 264, 267], it is good practice to differentiate "empty" values from "missing" ones. Empty values do not have a comparable real-world value. Missing values, however, do have underlying values, they simply weren't recorded. The author does not recommend ignoring the record with the missing value, since it would mean wasting the data stored in the other fields of that record. These fields may contain relevant information. Substituting the value means, that the record can be used. One of the problems with not having these values, is that this missing information content (e.g. predictive or inferential) could be carried by the pattern. Another consideration is how to substitute the missing value, without adding bias to the dataset. An inadequately chosen replacement value could distort the dataset, by adding data which doesn't exist in the real world. A crucial focus is reserving the relationship between variables. Substitution of values, if not suitable, may disrupt the between-variable variability, thus hiding or distorting patterns in the data. 

% \textcite{DataMiningAndPredictiveAnalytics}[23, 25] give an example, of how replacing missing values can lead to invalid results. The authors experimented with a database of cars. Substituting a missing brand with a random value (here "Japan") led to a car, that didn't even exist. Data imputation takes into account the other attributes stored in the record and from these, calculates what the missing value would most likely be. Larose and Larose suggest, that the value can be replaced, either with a constant determined by the data analyst, with a field mean (for numerical values) or mode (for categorical values), with a random value, or with imputed values based on the different features of the record.  
% \textcite{dataPreparationForDataMining}[260, 267-269] points out, that regression can be used to find supplement values. When using regression (e.g. linear regression), one can calculate a value with the help of another given value. There are several different methods to replace missing values. Some of which promise to generate more information. Such methods however are computationally complex.  


%%for more on data imputation - see chapter 27 

% \textcite{DataMiningAndPredictiveAnalytics}[25] 
% Another step in data preprocessing is identifying missclassifications. An example given by the authors, is classifying a record as USA instead of US, or France instead of Europe. These classes only contained one record in comparison to the other more frequently used classes.

%page 27 - chapter measures of center and spread - not sure if need - mean, median and mode

\subsubsection{Normalisation}
\label{section:Normalisation}


% \textcite{DataMiningAndPredictiveAnalytics}[130-31] 
% In some data mining algorithms, variables with higher ranges can unjustly influence the results, having more influence than smaller ones. Therefore, the authors recommend to normalise numerical data. 

%THIS IS ALSO EXPLAINED BY DATA PROCESSING IN DATA MINING ON PAGE 47
\textcite{han2011data}[105-106] describe normalisation as giving the attributes of a dataset equal weight. For example, it can transform the data to fall in a smaller, common range (e.g. [-1, 1]). It therefore hinders variables with large ranges from outweighing ones with smaller ranges. People's income would, for instance, have a larger range than binary attributes. 

% %data mining concepts and techniques
\textcite{dataPreprocessingInDataMining}[46-48] review the following normalisation methods: \textit{Min-Max Normalization}, \textit{Z-score Normalization}, and \textit{Decimal Scaling Normalization}. For the following examples, \textit{A} is a numerical attribute from a dataset, a single value of this attribute is represented with \textit{v}:
\begin{itemize}
  \item Min-max normalization scales the original numerical values to a newly defined range, with a new minimum (\textit{newMin\textsubscript{A}}) and maximum (\textit{newMax\textsubscript{A}}) (e.g. 0.0 and 1.0). The original minimum and maximum values found in \textit{A} are presented as \textit{min\textsubscript{A}} and \textit{max\textsubscript{A}} respectively:
  \[
    v' = \frac{v - min_A}{max_A - min_A}(newMax_A - newMin_A) + newMin_A
  \]
  The intervals [0, 1] and [-1, 1] are common intervals for normalisation. 
  % If new data is added, that isn't within the min and max of \textit{A} range, an "out-of-bounds" error will occur.

  \item Z-score (or zero-mean) normalization normalises the values using the mean (\textit{\={A}}) and standard deviation \textit{$\sigma$\textsubscript{A}} of the values \textit{A}.
  \[
    v' = \frac{v - \overline{A}}{\sigma_A}
  \]
  After this transformation, the mean equals zero and the standard deviation is one. The advantages of this normalisation method take effect, when the min and max values of \textit{A} are not known, or when there are outliers that could bias the min-max method.

  \item The decimal scaling method moves the decimal point enough spaces, so that the maximum absolute attribute value of \textit{A} is below one. The smallest required number of digits to move the decimal point, so that the largest absolute number in \textit{A} is below zero, is represented by \textit{j}:
  \[
    v' = \frac{v}{10^j}
  \]
\end{itemize}
